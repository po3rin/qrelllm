{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gokart\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "from qrelllm.llm.vertex import GenerateTestCollectionWithVertexAIBatch\n",
    "from qrelllm.queries import LoadQueries\n",
    "from qrelllm.format import clean_json\n",
    "from qrelllm.upload.gcs import UploadBatchForGCS\n",
    "from qrelllm.upload.openai import UploadOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project = os.getenv(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "location = os.getenv(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    bucket_name = \"llm-testcollection\"\n",
    "\n",
    "    queries = LoadQueries(csv_file_path=\"../data/queries.csv\")\n",
    "    upload_task = UploadBatchForGCS(\n",
    "        queries=queries,\n",
    "        bucket_name=bucket_name,\n",
    "        destination_blob_name=\"prompt/prompts.jsonl\",\n",
    "    )\n",
    "    generate_search_dataset_task = GenerateTestCollectionWithVertexAIBatch(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        destination_uri_prefix=f\"gs://{bucket_name}/result\",\n",
    "        upload_task=upload_task,\n",
    "    )\n",
    "    gokart.build(generate_search_dataset_task)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gcsから結果ロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_json_to_series(d):\n",
    "    keys, values = zip(*[item for dct in d for item in dct.items()])\n",
    "    df = pd.Series(values, index=keys)\n",
    "    return df[[\"content\"]]\n",
    "\n",
    "\n",
    "def json_to_series(d):\n",
    "    keys, values = zip(*[(dct[\"label\"], dct[\"value\"]) for dct in json.loads(d)])\n",
    "    df = pd.Series(values, index=keys)\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    # \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # # The ID of your GCS bucket\n",
    "    # # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # # The ID of your GCS object\n",
    "    # # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # # The path to which the file should be downloaded\n",
    "    # # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "            source_blob_name, bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pd.read_json(destination_file_name, orient=\"records\", lines=True)\n",
    "    df = pd.concat([df, df[\"predictions\"].apply(list_json_to_series)], axis=1)\n",
    "    df[\"content\"] = df[\"content\"].apply(clean_json)\n",
    "    print(df[\"content\"])\n",
    "    result = pd.concat([df, df[\"content\"].apply(json_to_series)], axis=1)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "bucket_name = \"llm-testcollection\"\n",
    "source_blob_name = (\n",
    "    \"result/prediction-model-XXXX/000000000000.jsonl\"\n",
    ")\n",
    "download_blob(bucket_name, source_blob_name, \"../data/batch-result.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
